#include <iostream>
#include <vector>
#include <cmath>
#include <random>
#include <string>
#include <map>
#include <algorithm>

// –°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π Transformer –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
class Transformer {
private:
    int vocab_size;
    int d_model;
    int nhead;
    int num_layers;
    int max_seq_len;
    
    // –°–ª–æ–≤–∞—Ä—å —Ç–æ–∫–µ–Ω–æ–≤
    std::map<std::string, int> word_to_id;
    std::map<int, std::string> id_to_word;
    int next_token_id = 0;
    
    // –í–µ—Å–∞ –º–æ–¥–µ–ª–∏
    std::vector<std::vector<std::vector<float>>> attention_weights;
    std::vector<std::vector<float>> feed_forward_weights;
    std::vector<std::vector<float>> embeddings;
    std::vector<std::vector<float>> position_embeddings;
    
public:
    Transformer(int vocab_size, int d_model = 512, int nhead = 8, int num_layers = 6, int max_seq_len = 1024) 
        : vocab_size(vocab_size), d_model(d_model), nhead(nhead), num_layers(num_layers), max_seq_len(max_seq_len) {
        
        // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ—Å–∞
        initializeWeights();
        
        // –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å
        createBasicVocabulary();
    }
    
    void initializeWeights() {
        std::random_device rd;
        std::mt19937 gen(rd());
        std::normal_distribution<float> dist(0.0, 0.1);
        
        // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        embeddings.resize(vocab_size, std::vector<float>(d_model));
        for(int i = 0; i < vocab_size; i++) {
            for(int j = 0; j < d_model; j++) {
                embeddings[i][j] = dist(gen);
            }
        }
        
        // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        position_embeddings.resize(max_seq_len, std::vector<float>(d_model));
        for(int i = 0; i < max_seq_len; i++) {
            for(int j = 0; j < d_model; j++) {
                position_embeddings[i][j] = dist(gen);
            }
        }
        
        // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è
        attention_weights.resize(num_layers, std::vector<std::vector<float>>(d_model, std::vector<float>(d_model)));
        for(int layer = 0; layer < num_layers; layer++) {
            for(int i = 0; i < d_model; i++) {
                for(int j = 0; j < d_model; j++) {
                    attention_weights[layer][i][j] = dist(gen);
                }
            }
        }
        
        // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º feed-forward –≤–µ—Å–∞
        feed_forward_weights.resize(num_layers, std::vector<float>(d_model * 4));
        for(int layer = 0; layer < num_layers; layer++) {
            for(int i = 0; i < d_model * 4; i++) {
                feed_forward_weights[layer][i] = dist(gen);
            }
        }
    }
    
    void createBasicVocabulary() {
        // –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ
        std::vector<std::string> basic_words = {
            "–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä", "–∫–∞–∫", "–¥–µ–ª–∞", "—á—Ç–æ", "—Ç—ã", "–¥–µ–ª–∞–µ—à—å", "—Å–µ–≥–æ–¥–Ω—è",
            "hello", "world", "how", "are", "you", "today", "what", "do",
            "—è", "–º—ã", "–æ–Ω", "–æ–Ω–∞", "–æ–Ω–∏", "—ç—Ç–æ", "—Ç–æ", "–≤–æ—Ç",
            "i", "we", "he", "she", "they", "this", "that", "here",
            "–¥–∞", "–Ω–µ—Ç", "—Ö–æ—Ä–æ—à–æ", "–ø–ª–æ—Ö–æ", "–¥–∞–≤–∞–π", "–ø–æ–π–¥–µ–º", "–∏–¥–µ–º",
            "yes", "no", "good", "bad", "let's", "go", "come",
            "—Å–ø–∞—Å–∏–±–æ", "–ø–æ–∂–∞–ª—É–π—Å—Ç–∞", "–∏–∑–≤–∏–Ω–∏", "–ø—Ä–æ—Å—Ç–∏", "–ø–æ–∫–∞",
            "thank", "you", "please", "sorry", "bye", "goodbye"
        };
        
        for(const auto& word : basic_words) {
            if(word_to_id.find(word) == word_to_id.end()) {
                word_to_id[word] = next_token_id;
                id_to_word[next_token_id] = word;
                next_token_id++;
            }
        }
        
        // –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        word_to_id["<PAD>"] = next_token_id++;
        word_to_id["<START>"] = next_token_id++;
        word_to_id["<END>"] = next_token_id++;
        word_to_id["<UNK>"] = next_token_id++;
        
        id_to_word[next_token_id-4] = "<PAD>";
        id_to_word[next_token_id-3] = "<START>";
        id_to_word[next_token_id-2] = "<END>";
        id_to_word[next_token_id-1] = "<UNK>";
    }
    
    std::vector<int> tokenize(const std::string& text) {
        std::vector<int> tokens;
        std::string word = "";
        
        for(char c : text) {
            if(c == ' ' || c == '.' || c == ',' || c == '!' || c == '?') {
                if(!word.empty()) {
                    if(word_to_id.find(word) != word_to_id.end()) {
                        tokens.push_back(word_to_id[word]);
                    } else {
                        tokens.push_back(word_to_id["<UNK>"]);
                    }
                    word = "";
                }
            } else {
                word += std::tolower(c);
            }
        }
        
        if(!word.empty()) {
            if(word_to_id.find(word) != word_to_id.end()) {
                tokens.push_back(word_to_id[word]);
            } else {
                tokens.push_back(word_to_id["<UNK>"]);
            }
        }
        
        return tokens;
    }
    
    std::string detokenize(const std::vector<int>& tokens) {
        std::string result = "";
        for(int token : tokens) {
            if(id_to_word.find(token) != id_to_word.end()) {
                if(!result.empty()) result += " ";
                result += id_to_word[token];
            }
        }
        return result;
    }
    
    std::vector<float> attention(const std::vector<float>& query, const std::vector<float>& key, const std::vector<float>& value) {
        std::vector<float> result(d_model, 0.0f);
        
        // –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (–±–µ–∑ softmax –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏)
        for(int i = 0; i < d_model; i++) {
            result[i] = query[i] * key[i] + value[i];
        }
        
        return result;
    }
    
    std::vector<float> feedForward(const std::vector<float>& input, int layer) {
        std::vector<float> result(d_model, 0.0f);
        
        // –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π feed-forward —Å–ª–æ–π
        for(int i = 0; i < d_model; i++) {
            result[i] = input[i] * feed_forward_weights[layer][i % (d_model * 4)];
        }
        
        return result;
    }
    
    std::vector<int> generate(const std::string& prompt, int max_length = 50) {
        std::vector<int> input_tokens = tokenize(prompt);
        std::vector<int> generated = input_tokens;
        
        std::random_device rd;
        std::mt19937 gen(rd());
        std::uniform_real_distribution<float> dist(0.0, 1.0);
        
        for(int step = 0; step < max_length; step++) {
            // –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            std::vector<std::vector<float>> embeddings_seq;
            for(int i = 0; i < std::min((int)generated.size(), max_seq_len); i++) {
                int token_id = generated[i];
                if(token_id < vocab_size) {
                    embeddings_seq.push_back(embeddings[token_id]);
                }
            }
            
            if(embeddings_seq.empty()) break;
            
            // –ü—Ä–æ—Ö–æ–¥–∏–º —á–µ—Ä–µ–∑ —Å–ª–æ–∏ Transformer
            std::vector<float> current = embeddings_seq.back(); // –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω
            
            for(int layer = 0; layer < num_layers; layer++) {
                // Self-attention
                current = attention(current, current, current);
                
                // Feed-forward
                current = feedForward(current, layer);
            }
            
            // –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
            int next_token = 0;
            float max_val = -1000.0f;
            
            for(int i = 0; i < vocab_size; i++) {
                float score = 0.0f;
                for(int j = 0; j < d_model; j++) {
                    score += current[j] * embeddings[i][j];
                }
                
                if(score > max_val) {
                    max_val = score;
                    next_token = i;
                }
            }
            
            generated.push_back(next_token);
            
            // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ END —Ç–æ–∫–µ–Ω–µ
            if(next_token == word_to_id["<END>"]) break;
        }
        
        return generated;
    }
    
    void printVocabulary() {
        std::cout << "–°–ª–æ–≤–∞—Ä—å —Ç–æ–∫–µ–Ω–æ–≤:" << std::endl;
        for(const auto& pair : word_to_id) {
            std::cout << pair.first << " -> " << pair.second << std::endl;
        }
    }
};

int main() {
    std::cout << "üöÄ –°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π Transformer –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—É—â–µ–Ω!" << std::endl;
    
    // –°–æ–∑–¥–∞–µ–º Transformer
    Transformer transformer(1000, 512, 8, 6, 1024);
    
    std::cout << "\nüìö –°–ª–æ–≤–∞—Ä—å:" << std::endl;
    transformer.printVocabulary();
    
    // –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
    std::string prompt = "–ø—Ä–∏–≤–µ—Ç –∫–∞–∫ –¥–µ–ª–∞";
    std::cout << "\nüí¨ –ü—Ä–æ–º–ø—Ç: " << prompt << std::endl;
    
    std::vector<int> generated = transformer.generate(prompt, 20);
    std::string result = transformer.detokenize(generated);
    
    std::cout << "ü§ñ –û—Ç–≤–µ—Ç: " << result << std::endl;
    
    std::cout << "\n‚úÖ Transformer —Ä–∞–±–æ—Ç–∞–µ—Ç —É—Å–ø–µ—à–Ω–æ!" << std::endl;
    
    return 0;
}
